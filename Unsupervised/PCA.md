# PCA
- ğŸ‘[æ©Ÿå™¨/çµ±è¨ˆå­¸ç¿’:ä¸»æˆåˆ†åˆ†æ(Principal Component Analysis, PCA)](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71)
- [PCA : the basics - simply explained](https://www.youtube.com/watch?v=dz8imS1vwIM)
- ğŸ‘[PCA : the math - step-by-step with a simple example](https://www.youtube.com/watch?v=S51bTyIwxFs)
- [Principal Component Analysis with Python](https://www.geeksforgeeks.org/principal-component-analysis-with-python/)
- [Lecture 15.1 â€” From PCA to autoencoders â€” [ Deep Learning | Geoffrey Hinton | UofT ]](https://www.youtube.com/watch?v=PSOt7u8u23w&t=48s)
- [ä¸–ä¸Šæœ€ç”Ÿå‹•çš„ PCAï¼šç›´è§€ç†è§£ä¸¦æ‡‰ç”¨ä¸»æˆåˆ†åˆ†æ](https://leemeng.tw/essence-of-principal-component-analysis.html)

# å¢é‡ PCA
- ç•¶è¦åˆ†è§£çš„è³‡æ–™é›†å¤ªå¤§è€Œç„¡æ³•æ”¾å…¥è¨˜æ†¶é«”æ™‚ï¼Œé€šå¸¸ä½¿ç”¨å¢é‡ä¸»æˆåˆ†åˆ†æ (IPCA) ã€‚
- IPCA ä½¿ç”¨èˆ‡è¼¸å…¥è³‡æ–™æ¨£æœ¬æ•¸é‡ç„¡é—œçš„è¨˜æ†¶é«”é‡ç‚ºè¼¸å…¥è³‡æ–™æ§‹å»ºä½ç§©è¿‘ä¼¼ã€‚
- å®ƒä»ç„¶ä¾è³´æ–¼è¼¸å…¥è³‡æ–™ç‰¹å¾µï¼Œä½†æ”¹è®Šæ‰¹é‡å¤§å°å¯ä»¥æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨ã€‚

# [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)
- ğŸ‘[Day18-Scikit-learnä»‹ç´¹(10)_ Principal Component Analysis](https://ithelp.ithome.com.tw/articles/10206243)
- n_componentsï¼šè¦ä¿ç•™çµ„ä»¶çš„æ•¸é‡
- pca.n_components_æŸ¥çœ‹ä¿ç•™çš„çµ„ä»¶æ•¸
- pca.explained_variance_ è§£é‡‹å¹³æ–¹å·®
```python

import numpy as np
from sklearn.decomposition import PCA

X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

pca = PCA(n_components=2)

pca.fit(X)

print(pca.explained_variance_ratio_)

print(pca.singular_values_)
```

# å®˜æ–¹ç¯„ä¾‹[Principal components analysis (PCA)](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html#sphx-glr-auto-examples-decomposition-plot-pca-3d-py) 
```python

# -*- coding: utf-8 -*-
"""
=========================================================
Principal components analysis (PCA)
=========================================================

These figures aid in illustrating how a point cloud
can be very flat in one direction--which is where PCA
comes in to choose a direction that is not flat.

"""

# Authors: Gael Varoquaux
#          Jaques Grobler
#          Kevin Hughes
# License: BSD 3 clause

# %%
# Create the data
# ---------------

import numpy as np

from scipy import stats

e = np.exp(1)
np.random.seed(4)


def pdf(x):
    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x) + stats.norm(scale=4 / e).pdf(x))


y = np.random.normal(scale=0.5, size=(30000))
x = np.random.normal(scale=0.5, size=(30000))
z = np.random.normal(scale=0.1, size=len(x))

density = pdf(x) * pdf(y)
pdf_z = pdf(5 * z)

density *= pdf_z

a = x + y
b = 2 * y
c = a - b + z

norm = np.sqrt(a.var() + b.var())
a /= norm
b /= norm


# %%
# Plot the figures
# ----------------

from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

# unused but required import for doing 3d projections with matplotlib < 3.2
import mpl_toolkits.mplot3d  # noqa: F401


def plot_figs(fig_num, elev, azim):
    fig = plt.figure(fig_num, figsize=(4, 3))
    plt.clf()
    ax = fig.add_subplot(111, projection="3d", elev=elev, azim=azim)
    ax.set_position([0, 0, 0.95, 1])

    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker="+", alpha=0.4)
    Y = np.c_[a, b, c]

    # Using SciPy's SVD, this would be:
    # _, pca_score, Vt = scipy.linalg.svd(Y, full_matrices=False)

    pca = PCA(n_components=3)
    pca.fit(Y)
    V = pca.components_.T

    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V
    x_pca_plane = np.r_[x_pca_axis[:2], -x_pca_axis[1::-1]]
    y_pca_plane = np.r_[y_pca_axis[:2], -y_pca_axis[1::-1]]
    z_pca_plane = np.r_[z_pca_axis[:2], -z_pca_axis[1::-1]]
    x_pca_plane.shape = (2, 2)
    y_pca_plane.shape = (2, 2)
    z_pca_plane.shape = (2, 2)
    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)
    ax.xaxis.set_ticklabels([])
    ax.yaxis.set_ticklabels([])
    ax.zaxis.set_ticklabels([])


elev = -40
azim = -80
plot_figs(1, elev, azim)

elev = 30
azim = 20
plot_figs(2, elev, azim)

plt.show()
```
# å®˜æ–¹ç¯„ä¾‹ [PCA example with Iris Data-set](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py)
```python
import numpy as np
import matplotlib.pyplot as plt


from sklearn import decomposition
from sklearn import datasets

# unused but required import for doing 3d projections with matplotlib < 3.2
import mpl_toolkits.mplot3d  # noqa: F401

np.random.seed(5)

iris = datasets.load_iris()
X = iris.data
y = iris.target

fig = plt.figure(1, figsize=(4, 3))
plt.clf()

ax = fig.add_subplot(111, projection="3d", elev=48, azim=134)
ax.set_position([0, 0, 0.95, 1])


plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

for name, label in [("Setosa", 0), ("Versicolour", 1), ("Virginica", 2)]:
    ax.text3D(
        X[y == label, 0].mean(),
        X[y == label, 1].mean() + 1.5,
        X[y == label, 2].mean(),
        name,
        horizontalalignment="center",
        bbox=dict(alpha=0.5, edgecolor="w", facecolor="w"),
    )
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor="k")

ax.xaxis.set_ticklabels([])
ax.yaxis.set_ticklabels([])
ax.zaxis.set_ticklabels([])

plt.show()
```
# åƒè€ƒè³‡è¨Š
